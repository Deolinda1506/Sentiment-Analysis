{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcff1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re  \n",
    "import emoji\n",
    "from sklearn.utils.class_weight import compute_class_weight  # Missing import!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3524a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (74681, 4)\n",
      "Validation set shape: (999, 4)\n",
      "\n",
      "Training set info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74681 entries, 0 to 74680\n",
      "Data columns (total 4 columns):\n",
      " #   Column                                                 Non-Null Count  Dtype \n",
      "---  ------                                                 --------------  ----- \n",
      " 0   2401                                                   74681 non-null  int64 \n",
      " 1   Borderlands                                            74681 non-null  object\n",
      " 2   Positive                                               74681 non-null  object\n",
      " 3   im getting on borderlands and i will murder you all ,  73995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n",
      "None\n",
      "\n",
      "Class distribution in training:\n",
      "Column names in training set:\n",
      "['2401', 'Borderlands', 'Positive', 'im getting on borderlands and i will murder you all ,']\n",
      "\n",
      "First few rows of training set:\n",
      "   2401  Borderlands  Positive  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "  im getting on borderlands and i will murder you all ,  \n",
      "0  I am coming to the borders and I will kill you...     \n",
      "1  im getting on borderlands and i will kill you ...     \n",
      "2  im coming on borderlands and i will murder you...     \n",
      "3  im getting on borderlands 2 and i will murder ...     \n",
      "4  im getting into borderlands and i can murder y...     \n",
      "\n",
      "Column names in validation set:\n",
      "['3364', 'Facebook', 'Irrelevant', 'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ğŸ¤£']\n",
      "\n",
      "First few rows of validation set:\n",
      "   3364   Facebook Irrelevant  \\\n",
      "0   352     Amazon    Neutral   \n",
      "1  8312  Microsoft   Negative   \n",
      "2  4371      CS-GO   Negative   \n",
      "3  4433     Google    Neutral   \n",
      "4  6273       FIFA   Negative   \n",
      "\n",
      "  I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ğŸ¤£  \n",
      "0  BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
      "1  @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
      "2  CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
      "3  Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
      "4  Hi @EAHelp Iâ€™ve had Madeleine McCann in my cel...                                                                                                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_df = pd.read_csv('/home/chris/Documents/Sentiment-Analysis/Datasets/twitter_training.csv')\n",
    "val_df =pd.read_csv('/home/chris/Documents/Sentiment-Analysis/Datasets/twitter_validation.csv')\n",
    "\n",
    "# Basic exploration\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "print(\"\\nTraining set info:\")\n",
    "print(train_df.info())\n",
    "print(\"\\nClass distribution in training:\")\n",
    "# First, let's see what columns you actually have\n",
    "print(\"Column names in training set:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\nFirst few rows of training set:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nColumn names in validation set:\")\n",
    "print(val_df.columns.tolist())\n",
    "print(\"\\nFirst few rows of validation set:\")\n",
    "print(val_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34e8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in training set:\n",
      "2401                                                       0\n",
      "Borderlands                                                0\n",
      "Positive                                                   0\n",
      "im getting on borderlands and i will murder you all ,    686\n",
      "dtype: int64\n",
      "\n",
      "Missing values in validation set:\n",
      "3364                                                                                                                                                                                                                                                  0\n",
      "Facebook                                                                                                                                                                                                                                              0\n",
      "Irrelevant                                                                                                                                                                                                                                            0\n",
      "I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ğŸ¤£    0\n",
      "dtype: int64\n",
      "\n",
      "Training set dtypes:\n",
      "2401                                                      int64\n",
      "Borderlands                                              object\n",
      "Positive                                                 object\n",
      "im getting on borderlands and i will murder you all ,    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in training set:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in validation set:\")\n",
    "print(val_df.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nTraining set dtypes:\")\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af39b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set after renaming:\n",
      "  sentiment                                               text\n",
      "0  Positive  I am coming to the borders and I will kill you...\n",
      "1  Positive  im getting on borderlands and i will kill you ...\n",
      "2  Positive  im coming on borderlands and i will murder you...\n",
      "3  Positive  im getting on borderlands 2 and i will murder ...\n",
      "4  Positive  im getting into borderlands and i can murder y...\n",
      "\n",
      "Validation set after renaming:\n",
      "  sentiment                                               text\n",
      "0   Neutral  BBC News - Amazon boss Jeff Bezos rejects clai...\n",
      "1  Negative  @Microsoft Why do I pay for WORD when it funct...\n",
      "2  Negative  CSGO matchmaking is so full of closet hacking,...\n",
      "3   Neutral  Now the President is slapping Americans in the...\n",
      "4  Negative  Hi @EAHelp Iâ€™ve had Madeleine McCann in my cel...\n",
      "\n",
      "Unique sentiments in training:\n",
      "sentiment\n",
      "Negative      22542\n",
      "Positive      20831\n",
      "Neutral       18318\n",
      "Irrelevant    12990\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique sentiments in validation:\n",
      "sentiment\n",
      "Neutral       285\n",
      "Positive      277\n",
      "Negative      266\n",
      "Irrelevant    171\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for consistency\n",
    "train_df.columns = ['id', 'topic', 'sentiment', 'text']\n",
    "val_df.columns = ['id', 'topic', 'sentiment', 'text']\n",
    "\n",
    "print(\"Training set after renaming:\")\n",
    "print(train_df[['sentiment', 'text']].head())\n",
    "\n",
    "print(\"\\nValidation set after renaming:\")\n",
    "print(val_df[['sentiment', 'text']].head())\n",
    "\n",
    "# Check unique sentiment values\n",
    "print(\"\\nUnique sentiments in training:\")\n",
    "print(train_df['sentiment'].value_counts())\n",
    "\n",
    "print(\"\\nUnique sentiments in validation:\")\n",
    "print(val_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51daf1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set before removing missing: 74681\n",
      "Training set after removing missing: 73995\n",
      "Validation set before removing missing: 999\n",
      "Validation set after removing missing: 999\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing text\n",
    "print(f\"Training set before removing missing: {len(train_df)}\")\n",
    "train_df = train_df.dropna(subset=['text'])\n",
    "print(f\"Training set after removing missing: {len(train_df)}\")\n",
    "\n",
    "print(f\"Validation set before removing missing: {len(val_df)}\")\n",
    "val_df = val_df.dropna(subset=['text'])\n",
    "print(f\"Validation set after removing missing: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc4d63b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning texts...\n",
      "Label mapping: {'Irrelevant': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3}\n",
      "Training labels distribution: [12875 22358 18108 20654]\n",
      "Validation labels distribution: [171 266 285 277]\n"
     ]
    }
   ],
   "source": [
    "# Twitter text cleaning function\n",
    "def clean_twitter_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text)\n",
    "    \n",
    "    # Remove user mentions but keep text\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Handle emojis - convert to text description\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s!?,.]', '', text)\n",
    "    \n",
    "    # Handle repeated characters (e.g., \"loooove\" -> \"love\")\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Cleaning texts...\")\n",
    "train_texts = train_df['text'].apply(clean_twitter_text)\n",
    "val_texts = val_df['text'].apply(clean_twitter_text)\n",
    "\n",
    "# Prepare labels for 4-class classification\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['sentiment'])\n",
    "y_train = le.transform(train_df['sentiment'])\n",
    "y_val = le.transform(val_df['sentiment'])\n",
    "\n",
    "print(f\"Label mapping: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "print(f\"Training labels distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Validation labels distribution: {np.bincount(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cadda7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length analysis:\n",
      "Max length: 166\n",
      "Average length: 18.48\n",
      "95th percentile: 47.0\n",
      "\n",
      "Final data shapes:\n",
      "X_train: (73995, 60), y_train: (73995,)\n",
      "X_val: (999, 60), y_val: (999,)\n",
      "Vocabulary size: 37107\n"
     ]
    }
   ],
   "source": [
    "# Tokenization with larger vocabulary for Twitter data\n",
    "tokenizer = Tokenizer(num_words=15000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Convert to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "# Analyze sequence lengths\n",
    "train_lens = [len(seq) for seq in train_sequences]\n",
    "print(f\"Sequence length analysis:\")\n",
    "print(f\"Max length: {max(train_lens)}\")\n",
    "print(f\"Average length: {np.mean(train_lens):.2f}\")\n",
    "print(f\"95th percentile: {np.percentile(train_lens, 95)}\")\n",
    "\n",
    "# Set max length based on analysis\n",
    "max_length = 60  # Covers most tweets\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "X_val = pad_sequences(val_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f\"\\nFinal data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee067471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model for 4 classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-10-02 20:18:53.514147: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def create_lstm_model(vocab_size, num_classes=4, embedding_dim=100, lstm_units=64):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "        Dropout(0.4),\n",
    "        Bidirectional(LSTM(lstm_units//2)),\n",
    "        Dropout(0.4),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "vocab_size = min(len(tokenizer.word_index) + 1, 15000)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "print(f\"Creating model for {num_classes} classes...\")\n",
    "model = create_lstm_model(vocab_size, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fdbee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights for handling imbalance:\n",
      "  Irrelevant: 1.44\n",
      "  Negative: 0.83\n",
      "  Neutral: 1.02\n",
      "  Positive: 0.90\n",
      "Starting training...\n",
      "Epoch 1/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 232ms/step - accuracy: 0.4939 - loss: 1.1502 - val_accuracy: 0.8699 - val_loss: 0.4029 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 252ms/step - accuracy: 0.8201 - loss: 0.5135 - val_accuracy: 0.9109 - val_loss: 0.2534 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 267ms/step - accuracy: 0.8761 - loss: 0.3516 - val_accuracy: 0.9389 - val_loss: 0.2023 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 226ms/step - accuracy: 0.9008 - loss: 0.2729 - val_accuracy: 0.9459 - val_loss: 0.2089 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 302ms/step - accuracy: 0.9165 - loss: 0.2352 - val_accuracy: 0.9499 - val_loss: 0.1927 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 260ms/step - accuracy: 0.9253 - loss: 0.2089 - val_accuracy: 0.9510 - val_loss: 0.1862 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 245ms/step - accuracy: 0.9342 - loss: 0.1763 - val_accuracy: 0.9590 - val_loss: 0.2085 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 189ms/step - accuracy: 0.9417 - loss: 0.1582 - val_accuracy: 0.9540 - val_loss: 0.2402 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.9460 - loss: 0.1416\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 218ms/step - accuracy: 0.9460 - loss: 0.1416 - val_accuracy: 0.9520 - val_loss: 0.2397 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46814s\u001b[0m 40s/step - accuracy: 0.9537 - loss: 0.1195 - val_accuracy: 0.9590 - val_loss: 0.2571 - learning_rate: 2.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 139ms/step - accuracy: 0.9582 - loss: 0.1039 - val_accuracy: 0.9540 - val_loss: 0.3156 - learning_rate: 2.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9615 - loss: 0.0970\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m1157/1157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 136ms/step - accuracy: 0.9615 - loss: 0.0970 - val_accuracy: 0.9560 - val_loss: 0.3436 - learning_rate: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class weights for handling imbalance:\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    print(f\"  {class_name}: {class_weights[i]:.2f}\")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
