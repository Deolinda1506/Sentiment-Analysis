{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcff1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re  \n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3524a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (74681, 4)\n",
      "Validation set shape: (999, 4)\n",
      "\n",
      "Training set info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74681 entries, 0 to 74680\n",
      "Data columns (total 4 columns):\n",
      " #   Column                                                 Non-Null Count  Dtype \n",
      "---  ------                                                 --------------  ----- \n",
      " 0   2401                                                   74681 non-null  int64 \n",
      " 1   Borderlands                                            74681 non-null  object\n",
      " 2   Positive                                               74681 non-null  object\n",
      " 3   im getting on borderlands and i will murder you all ,  73995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n",
      "None\n",
      "\n",
      "Class distribution in training:\n",
      "Column names in training set:\n",
      "['2401', 'Borderlands', 'Positive', 'im getting on borderlands and i will murder you all ,']\n",
      "\n",
      "First few rows of training set:\n",
      "   2401  Borderlands  Positive  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "  im getting on borderlands and i will murder you all ,  \n",
      "0  I am coming to the borders and I will kill you...     \n",
      "1  im getting on borderlands and i will kill you ...     \n",
      "2  im coming on borderlands and i will murder you...     \n",
      "3  im getting on borderlands 2 and i will murder ...     \n",
      "4  im getting into borderlands and i can murder y...     \n",
      "\n",
      "Column names in validation set:\n",
      "['3364', 'Facebook', 'Irrelevant', 'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£']\n",
      "\n",
      "First few rows of validation set:\n",
      "   3364   Facebook Irrelevant  \\\n",
      "0   352     Amazon    Neutral   \n",
      "1  8312  Microsoft   Negative   \n",
      "2  4371      CS-GO   Negative   \n",
      "3  4433     Google    Neutral   \n",
      "4  6273       FIFA   Negative   \n",
      "\n",
      "  I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£  \n",
      "0  BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
      "1  @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
      "2  CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
      "3  Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
      "4  Hi @EAHelp Iâ€™ve had Madeleine McCann in my cel...                                                                                                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_df = pd.read_csv('../Datasets/twitter_training.csv')\n",
    "val_df =pd.read_csv('../Datasets/twitter_validation.csv')\n",
    "\n",
    "# Basic exploration\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "print(\"\\nTraining set info:\")\n",
    "print(train_df.info())\n",
    "print(\"\\nClass distribution in training:\")\n",
    "# First, let's see what columns you actually have\n",
    "print(\"Column names in training set:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\nFirst few rows of training set:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nColumn names in validation set:\")\n",
    "print(val_df.columns.tolist())\n",
    "print(\"\\nFirst few rows of validation set:\")\n",
    "print(val_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34e8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in training set:\n",
      "2401                                                       0\n",
      "Borderlands                                                0\n",
      "Positive                                                   0\n",
      "im getting on borderlands and i will murder you all ,    686\n",
      "dtype: int64\n",
      "\n",
      "Missing values in validation set:\n",
      "3364                                                                                                                                                                                                                                                  0\n",
      "Facebook                                                                                                                                                                                                                                              0\n",
      "Irrelevant                                                                                                                                                                                                                                            0\n",
      "I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£    0\n",
      "dtype: int64\n",
      "\n",
      "Training set dtypes:\n",
      "2401                                                      int64\n",
      "Borderlands                                              object\n",
      "Positive                                                 object\n",
      "im getting on borderlands and i will murder you all ,    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in training set:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in validation set:\")\n",
    "print(val_df.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nTraining set dtypes:\")\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af39b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set after renaming:\n",
      "  sentiment                                               text\n",
      "0  Positive  I am coming to the borders and I will kill you...\n",
      "1  Positive  im getting on borderlands and i will kill you ...\n",
      "2  Positive  im coming on borderlands and i will murder you...\n",
      "3  Positive  im getting on borderlands 2 and i will murder ...\n",
      "4  Positive  im getting into borderlands and i can murder y...\n",
      "\n",
      "Validation set after renaming:\n",
      "  sentiment                                               text\n",
      "0   Neutral  BBC News - Amazon boss Jeff Bezos rejects clai...\n",
      "1  Negative  @Microsoft Why do I pay for WORD when it funct...\n",
      "2  Negative  CSGO matchmaking is so full of closet hacking,...\n",
      "3   Neutral  Now the President is slapping Americans in the...\n",
      "4  Negative  Hi @EAHelp Iâ€™ve had Madeleine McCann in my cel...\n",
      "\n",
      "Unique sentiments in training:\n",
      "sentiment\n",
      "Negative      22542\n",
      "Positive      20831\n",
      "Neutral       18318\n",
      "Irrelevant    12990\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique sentiments in validation:\n",
      "sentiment\n",
      "Neutral       285\n",
      "Positive      277\n",
      "Negative      266\n",
      "Irrelevant    171\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for consistency\n",
    "train_df.columns = ['id', 'topic', 'sentiment', 'text']\n",
    "val_df.columns = ['id', 'topic', 'sentiment', 'text']\n",
    "\n",
    "print(\"Training set after renaming:\")\n",
    "print(train_df[['sentiment', 'text']].head())\n",
    "\n",
    "print(\"\\nValidation set after renaming:\")\n",
    "print(val_df[['sentiment', 'text']].head())\n",
    "\n",
    "# Check unique sentiment values\n",
    "print(\"\\nUnique sentiments in training:\")\n",
    "print(train_df['sentiment'].value_counts())\n",
    "\n",
    "print(\"\\nUnique sentiments in validation:\")\n",
    "print(val_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51daf1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set before removing missing: 74681\n",
      "Training set after removing missing: 73995\n",
      "Validation set before removing missing: 999\n",
      "Validation set after removing missing: 999\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing text\n",
    "print(f\"Training set before removing missing: {len(train_df)}\")\n",
    "train_df = train_df.dropna(subset=['text'])\n",
    "print(f\"Training set after removing missing: {len(train_df)}\")\n",
    "\n",
    "print(f\"Validation set before removing missing: {len(val_df)}\")\n",
    "val_df = val_df.dropna(subset=['text'])\n",
    "print(f\"Validation set after removing missing: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4d63b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning texts...\n",
      "Label mapping: {'Irrelevant': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3}\n",
      "Training labels distribution: [12875 22358 18108 20654]\n",
      "Validation labels distribution: [171 266 285 277]\n"
     ]
    }
   ],
   "source": [
    "# Twitter text cleaning function\n",
    "def clean_twitter_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text)\n",
    "    \n",
    "    # Remove user mentions but keep text\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Handle emojis - convert to text description\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s!?,.]', '', text)\n",
    "    \n",
    "    # Handle repeated characters (e.g., \"loooove\" -> \"love\")\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Cleaning texts...\")\n",
    "train_texts = train_df['text'].apply(clean_twitter_text)\n",
    "val_texts = val_df['text'].apply(clean_twitter_text)\n",
    "\n",
    "# Prepare labels for 4-class classification\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['sentiment'])\n",
    "y_train = le.transform(train_df['sentiment'])\n",
    "y_val = le.transform(val_df['sentiment'])\n",
    "\n",
    "print(f\"Label mapping: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "print(f\"Training labels distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Validation labels distribution: {np.bincount(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cadda7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length analysis:\n",
      "Max length: 166\n",
      "Average length: 18.48\n",
      "95th percentile: 47.0\n",
      "\n",
      "Final data shapes:\n",
      "X_train: (73995, 60), y_train: (73995,)\n",
      "X_val: (999, 60), y_val: (999,)\n",
      "Vocabulary size: 37107\n"
     ]
    }
   ],
   "source": [
    "# Tokenization with larger vocabulary for Twitter data\n",
    "tokenizer = Tokenizer(num_words=15000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Convert to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "# Analyze sequence lengths\n",
    "train_lens = [len(seq) for seq in train_sequences]\n",
    "print(f\"Sequence length analysis:\")\n",
    "print(f\"Max length: {max(train_lens)}\")\n",
    "print(f\"Average length: {np.mean(train_lens):.2f}\")\n",
    "print(f\"95th percentile: {np.percentile(train_lens, 95)}\")\n",
    "\n",
    "# Set max length based on analysis\n",
    "max_length = 60  # Covers most tweets\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "X_val = pad_sequences(val_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f\"\\nFinal data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee067471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
