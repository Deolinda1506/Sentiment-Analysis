{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c0dc42",
   "metadata": {},
   "source": [
    "[markdown]\n",
    "deep_learning_model_improved.py\n",
    "Improved, runnable notebook-style Python script for the DL portion of the assignment.\n",
    "Save this as a .py or paste into Jupyter as cells (cells separated by '# %%')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15fd1a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "Overview\n",
    "- Loads provided training and validation CSVs\n",
    "- Performs EDA with 4+ visualizations\n",
    "- Preprocesses text (cleaning, tokenization, stopword removal)\n",
    "- Creates two deep-learning models:\n",
    "    1) Embedding + BiLSTM\n",
    "    2) TF-IDF -> Dense network\n",
    "- Runs experiments varying hyperparameters\n",
    "- Produces evaluation metrics and plots\n",
    "- Saves experiment CSV and model artifacts\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c78e4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TF_AVAILABLE = True\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "except Exception as e:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow/Keras not available in this environment.\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d76bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Paths\n",
    "TRAIN_PATH = '../Datasets/twitter_training.csv'\n",
    "VALID_PATH = '../Datasets/twitter_validation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a382e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (74681, 4), Valid shape: (999, 4)\n",
      "   2401  Borderlands  Positive  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "  im getting on borderlands and i will murder you all ,  \n",
      "0  I am coming to the borders and I will kill you...     \n",
      "1  im getting on borderlands and i will kill you ...     \n",
      "2  im coming on borderlands and i will murder you...     \n",
      "3  im getting on borderlands 2 and i will murder ...     \n",
      "4  im getting into borderlands and i can murder y...     \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load data\n",
    "def load_datasets(train_path=TRAIN_PATH, valid_path=VALID_PATH):\n",
    "    train = pd.read_csv(train_path)\n",
    "    valid = pd.read_csv(valid_path)\n",
    "    return train, valid\n",
    "\n",
    "train_df, valid_df = load_datasets()\n",
    "print(f\"Train shape: {train_df.shape}, Valid shape: {valid_df.shape}\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "852357bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No label column found! Please check dataset columns.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing label column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLABEL_COL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo label column found! Please check dataset columns.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Detect text column\u001b[39;00m\n\u001b[32m     13\u001b[39m possible_text_cols = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m train_df.columns \u001b[38;5;28;01mif\u001b[39;00m c.lower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mreview\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtweet\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[31mValueError\u001b[39m: No label column found! Please check dataset columns."
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# EDA: Class distribution\n",
    "\n",
    "# Try to detect label column automatically\n",
    "possible_label_cols = [c for c in train_df.columns if c.lower() in ('label','sentiment','target')]\n",
    "if possible_label_cols:\n",
    "    LABEL_COL = possible_label_cols[0]\n",
    "    print(f\"Using label column: {LABEL_COL}\")\n",
    "else:\n",
    "    raise ValueError(\"No label column found! Please check dataset columns.\")\n",
    "\n",
    "# Detect text column\n",
    "possible_text_cols = [c for c in train_df.columns if c.lower() in ('text','review','content','tweet')]\n",
    "if possible_text_cols:\n",
    "    TEXT_COL = possible_text_cols[0]\n",
    "    print(f\"Using text column: {TEXT_COL}\")\n",
    "else:\n",
    "    raise ValueError(\"No text column found! Please check dataset columns.\")\n",
    "\n",
    "# Plot distribution\n",
    "class_counts = train_df[LABEL_COL].value_counts().sort_index()\n",
    "plt.figure(figsize=(6,4))\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Class distribution (train)')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485663e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# EDA: Review length distributions\n",
    "train_df['char_len'] = train_df[TEXT_COL].astype(str).apply(len)\n",
    "train_df['word_len'] = train_df[TEXT_COL].astype(str).apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(train_df['char_len'], bins=50)\n",
    "plt.title('Character length distribution')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(train_df['word_len'], bins=50)\n",
    "plt.title('Word count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24289b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# EDA: Top unigrams\n",
    "cv = CountVectorizer(stop_words='english', max_features=50)\n",
    "cv.fit(train_df[TEXT_COL].astype(str))\n",
    "uni = cv.transform(train_df[TEXT_COL].astype(str))\n",
    "uni_sum = np.array(uni.sum(axis=0)).flatten()\n",
    "words = cv.get_feature_names_out()\n",
    "top_idx = np.argsort(uni_sum)[-20:][::-1]\n",
    "plt.barh(np.arange(len(top_idx)), uni_sum[top_idx][::-1])\n",
    "plt.yticks(np.arange(len(top_idx)), words[top_idx][::-1])\n",
    "plt.title('Top 20 unigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# EDA: Top bigrams\n",
    "cv_bigram = CountVectorizer(ngram_range=(2,2), stop_words='english', max_features=50)\n",
    "cv_bigram.fit(train_df[TEXT_COL].astype(str))\n",
    "bi = cv_bigram.transform(train_df[TEXT_COL].astype(str))\n",
    "bi_sum = np.array(bi.sum(axis=0)).flatten()\n",
    "bi_words = cv_bigram.get_feature_names_out()\n",
    "topb_idx = np.argsort(bi_sum)[-20:][::-1]\n",
    "plt.barh(np.arange(len(topb_idx)), bi_sum[topb_idx][::-1])\n",
    "plt.yticks(np.arange(len(topb_idx)), bi_words[topb_idx][::-1])\n",
    "plt.title('Top 20 bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Preprocessing\n",
    "import html\n",
    "STOPWORDS = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(r'http\\S+|www\\S+', ' ', s)\n",
    "    s = re.sub(r'@\\w+', ' ', s)\n",
    "    s = re.sub(r'#', ' ', s)\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "train_df['clean_text'] = train_df[TEXT_COL].astype(str).apply(clean_text)\n",
    "valid_df['clean_text'] = valid_df[TEXT_COL].astype(str).apply(clean_text)\n",
    "print(train_df[['text','clean_text',LABEL_COL]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3093f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Prepare inputs: sequences\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBED_DIM = 100\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(train_df['clean_text'])\n",
    "    X_train_seq = tokenizer.texts_to_sequences(train_df['clean_text'])\n",
    "    X_valid_seq = tokenizer.texts_to_sequences(valid_df['clean_text'])\n",
    "    X_train_seq = pad_sequences(X_train_seq, maxlen=MAX_SEQ_LEN)\n",
    "    X_valid_seq = pad_sequences(X_valid_seq, maxlen=MAX_SEQ_LEN)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Vocab size:', len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fddaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# TF-IDF representation\n",
    "TFIDF_MAX_FEATURES = 20000\n",
    "tfidf = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['clean_text']).toarray()\n",
    "X_valid_tfidf = tfidf.transform(valid_df['clean_text']).toarray()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(train_df[LABEL_COL])\n",
    "Y_valid = le.transform(valid_df[LABEL_COL])\n",
    "print('Classes:', le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d35a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Model definitions\n",
    "if TF_AVAILABLE:\n",
    "    def build_lstm_model(vocab_size):\n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, EMBED_DIM, input_length=MAX_SEQ_LEN),\n",
    "            Bidirectional(LSTM(64)),\n",
    "            Dropout(0.5),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def build_cnn_model(vocab_size):\n",
    "        model = Sequential([\n",
    "            Embedding(vocab_size, EMBED_DIM, input_length=MAX_SEQ_LEN),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def build_dense_tfidf_model(input_dim):\n",
    "        model = Sequential([\n",
    "            Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "            Dropout(0.5),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735fab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Experiments grid\n",
    "experiments = []\n",
    "experiment_grid = [\n",
    "    {'model_type':'lstm','lr':1e-3,'batch_size':64,'optimizer':'adam','epochs':5},\n",
    "    {'model_type':'cnn','lr':1e-3,'batch_size':64,'optimizer':'rmsprop','epochs':5},\n",
    "    {'model_type':'tfidf_dense','lr':1e-3,'batch_size':64,'optimizer':'adam','epochs':5}\n",
    "]\n",
    "\n",
    "# Training loop placeholder (requires TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabe789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Evaluation helpers\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "def plot_confusion(cm, labels=None):\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels).plot(cmap=plt.cm.Blues)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a49de",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# Report and README instructions\n",
    "# - Generate PDF report with dataset, EDA visuals, preprocessing, models, experiments, evaluation, conclusions.\n",
    "# - Use `experiments_results.csv` for tables.\n",
    "# - GitHub repo should include code, report, README."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
